{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7124276",
   "metadata": {},
   "source": [
    "# Base example: How to Langchain\n",
    "This is a simple example of how to use Langchain to split python files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab0830c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PythonLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaLLM\n",
    "# from langchain_community.llms import CTransformers\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\"../capstone/S35-Capstone-Backend\",\n",
    "                         glob=\"*.py\", loader_cls=PythonLoader, recursive=True)\n",
    "# interpret information in the documents\n",
    "documents = loader.load()\n",
    "\n",
    "for i, v in enumerate(documents):\n",
    "    print(i, v)\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=256,\n",
    "#                                           chunk_overlap=20)\n",
    "# texts = splitter.split_documents(documents)\n",
    "\n",
    "# text_splitter = NLTKTextSplitter()\n",
    "# texts = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': \"cuda\"})\n",
    "\n",
    "# create and save the local database\n",
    "db = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "template = \"\"\"Use the following context to answer the user's question. \n",
    "Context: {context}\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "# load the language model\n",
    "llm = OllamaLLM(model=\"llama3.1:8b\",\n",
    "                num_predict=-1,\n",
    "                temperature=0.1)\n",
    "# llm = CTransformers(model='./llama-2-7b-chat.ggmlv3.q8_0.bin',\n",
    "#                     model_type='llama',\n",
    "#                     gpu_layers=50,\n",
    "#                     config={'max_new_tokens': 1024, 'temperature': 0.05})\n",
    "\n",
    "\n",
    "# prepare a version of the llm pre-loaded with the local content\n",
    "retriever = db.as_retriever(search_kwargs={'k': 5})\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['context', 'input'])\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "qa_llm = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "# qa_llm = RetrievalQA.from_chain_type(llm=llm,\n",
    "#                                      chain_type='stuff',\n",
    "#                                      retriever=retriever,\n",
    "#                                      return_source_documents=True,\n",
    "#                                      chain_type_kwargs={'prompt': prompt})\n",
    "\n",
    "# ask the AI chat about information in our local files\n",
    "\n",
    "while True:\n",
    "    inputP = input(\"What do you want to ask?\\n\")\n",
    "    output = qa_llm.invoke({\"input\": inputP})\n",
    "    print(output[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36d1d2",
   "metadata": {},
   "source": [
    "# Modified example: Custom Retriever\n",
    "\n",
    "Testing if using a custom AST based retriever to split makes it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb8c6880",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tree_sitter_python'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../ast_tokenizer/languages\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanguages\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython_ast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PythonASTDocumentLoader\n",
      "File \u001b[0;32m~/Documents/info_retrieval_codebase_rag/ast_tokenizer/languages/python_ast.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Reference for Langchain BaseLoader: https://python.langchain.com/docs/how_to/document_loader_custom/\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Refernce for Treesitter-Python: https://github.com/tree-sitter/tree-sitter-python/tree/master\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtree_sitter_python\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtspython\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtree_sitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language, Node, Parser\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Iterator, Union, Optional, List, Dict, Tuple\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tree_sitter_python'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(os.path.abspath('')), '../ast_tokenizer/languages')))\n",
    "from languages.python_ast import PythonASTDocumentLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9426b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG Kernel",
   "language": "python",
   "name": "rag-codebase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
