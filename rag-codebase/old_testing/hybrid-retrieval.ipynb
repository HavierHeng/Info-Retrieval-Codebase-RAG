{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7124276",
   "metadata": {},
   "source": [
    "# Base example: How to Langchain\n",
    "This is a simple example of how to use Langchain to split python files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0830c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv (Python 3.11.9)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PythonLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.retrievers import BM25Retriever\n",
    "# from langchain_community.llms import CTransformers\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\"../../ast_tokenizer\",\n",
    "                         glob=\"*.py\", loader_cls=PythonLoader, recursive=True)\n",
    "# interpret information in the documents\n",
    "documents = loader.load()\n",
    "\n",
    "for i, v in enumerate(documents):\n",
    "    print(i, v)\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=256,\n",
    "#                                           chunk_overlap=20)\n",
    "# texts = splitter.split_documents(documents)\n",
    "\n",
    "# text_splitter = NLTKTextSplitter()\n",
    "# texts = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': \"cuda\"})\n",
    "\n",
    "# create and save the local database\n",
    "db = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# do prompt engineering here\n",
    "template = \"\"\"Use the following context to answer the user's question. \n",
    "Context: {context}\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "# load the language model\n",
    "llm = OllamaLLM(model=\"llama3.1:8b\",\n",
    "                num_predict=-1,\n",
    "                temperature=0.1)\n",
    "# llm = CTransformers(model='./llama-2-7b-chat.ggmlv3.q8_0.bin',\n",
    "#                     model_type='llama',\n",
    "#                     gpu_layers=50,\n",
    "#                     config={'max_new_tokens': 1024, 'temperature': 0.05})\n",
    "\n",
    "\n",
    "# prepare a version of the llm pre-loaded with the local content\n",
    "retriever = db.as_retriever(search_kwargs={'k': 5})\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['context', 'input'])\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "qa_llm = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "# qa_llm = RetrievalQA.from_chain_type(llm=llm,\n",
    "#                                      chain_type='stuff',\n",
    "#                                      retriever=retriever,\n",
    "#                                      return_source_documents=True,\n",
    "#                                      chain_type_kwargs={'prompt': prompt})\n",
    "\n",
    "# ask the AI chat about information in our local files\n",
    "\n",
    "while True:\n",
    "    inputP = input(\"What do you want to ask?\\n\")\n",
    "    output = qa_llm.invoke({\"input\": inputP})\n",
    "    print(output[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36d1d2",
   "metadata": {},
   "source": [
    "# Modified example: Custom Retriever\n",
    "\n",
    "Testing if using a custom AST based retriever to split makes it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ec05cf9-5e80-47d0-a1a8-9f18db169b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/javier/miniforge3/envs/rag-codebase/lib/python313.zip', '/Users/javier/miniforge3/envs/rag-codebase/lib/python3.13', '/Users/javier/miniforge3/envs/rag-codebase/lib/python3.13/lib-dynload', '', '/Users/javier/miniforge3/envs/rag-codebase/lib/python3.13/site-packages', '/Users/javier/Documents/info_retrieval_codebase_rag/ast_tokenizer/languages/python_ast', '/Users/javier/Documents/info_retrieval_codebase_rag/ast_tokenizer/languages/python_ast', '/Users/javier/Documents/info_retrieval_codebase_rag/ast_tokenizer/languages/python_ast', '/Users/javier/Documents/info_retrieval_codebase_rag/ast_tokenizer/languages', '/Users/javier/Documents/info_retrieval_codebase_rag/ast_tokenizer/languages']\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(os.path.abspath('')), '../ast_tokenizer/languages')))\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb8c6880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from python_ast import PythonASTDocumentLoader\n",
    "from javascript_ast import JavascriptASTDocumentLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cb9426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\"../\",\n",
    "                         glob=\"*.py\", loader_cls=PythonASTDocumentLoader, recursive=True)\n",
    "# interpret information in the documents\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da8be853-c696-4662-9100-6b0005c4c426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'relative_path': '../testing/test_splitter.py', 'start_offset': 0, 'end_offset': 1162, 'block_type': 'others', 'block_name': 'Global Scope', 'block_args': [], 'parent_type': 'root', 'parent_name': 'root', 'functions_called': ['open(\"../../frontend/ui/ui.py\")', 'f.read()', 'RecursiveCharacterTextSplitter.from_language(\\n    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\\n)', 'python_splitter.create_documents([PYTHON_CODE])', 'print(\"\\\\n\\\\n\\\\n\")', 'print(\"TESTINGTESTINGTESTING\")', 'print(\"\\\\n\\\\n\\\\n\")', 'PythonLoader(\"mainFile.py\")', 'loader.load()', 'PythonSegmenter(PYTHON_CODE)', 'print(segmenter.is_valid())', 'segmenter.is_valid()', 'print(\"\\\\n Class Func\\\\n\")', 'print(segmenter.extract_functions_classes())', 'segmenter.extract_functions_classes()', 'print(\"\\\\n Simplified \\\\n\")', 'print(segmenter.simplify_code())', 'segmenter.simplify_code()'], 'docstrings': [], 'comments': ['for doc in python_docs:', 'Splitter splits per line or statement naively', 'print(type(doc), doc)', 'for doc in loader_docs:', 'PythonLoader has a large chunk size, but can load from files directly', 'Results in one huge document, that needs further recursive splitting', 'print(type(doc), doc)']}\n",
      "1 {'relative_path': '../testing/mainFile.py', 'start_offset': 0, 'end_offset': 2753, 'block_type': 'others', 'block_name': 'Global Scope', 'block_args': [], 'parent_type': 'root', 'parent_name': 'root', 'functions_called': ['DirectoryLoader(\"../capstone/S35-Capstone-Backend\",\\r\\n                         glob=\"*.py\", loader_cls=PythonLoader, recursive=True)', 'loader.load()', 'enumerate(documents)', 'print(i, v)', 'HuggingFaceEmbeddings(\\r\\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\\r\\n    model_kwargs={\\'device\\': \"cuda\"})', 'FAISS.from_documents(documents, embeddings)', 'OllamaLLM(model=\"llama3.1:8b\",\\r\\n                num_predict=-1,\\r\\n                temperature=0.1)', \"db.as_retriever(search_kwargs={'k': 5})\", \"PromptTemplate(\\r\\n    template=template,\\r\\n    input_variables=['context', 'input'])\", 'create_stuff_documents_chain(llm, prompt)', 'create_retrieval_chain(retriever, combine_docs_chain)', 'input(\"What do you want to ask?\\\\n\")', 'qa_llm.invoke({\"input\": inputP})', 'print(output[\"answer\"])'], 'docstrings': [], 'comments': ['from langchain_community.llms import CTransformers', 'interpret information in the documents', 'splitter = RecursiveCharacterTextSplitter(chunk_size=256,', 'chunk_overlap=20)', 'texts = splitter.split_documents(documents)', 'text_splitter = NLTKTextSplitter()', 'texts = text_splitter.split_documents(documents)', 'create and save the local database', 'load the language model', \"llm = CTransformers(model='./llama-2-7b-chat.ggmlv3.q8_0.bin',\", \"model_type='llama',\", 'gpu_layers=50,', \"config={'max_new_tokens': 1024, 'temperature': 0.05})\", 'prepare a version of the llm pre-loaded with the local content', 'qa_llm = RetrievalQA.from_chain_type(llm=llm,', \"chain_type='stuff',\", 'retriever=retriever,', 'return_source_documents=True,', \"chain_type_kwargs={'prompt': prompt})\", 'ask the AI chat about information in our local files']}\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(documents):\n",
    "    print(i, v.metadata[relative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e974a5-134f-4d2a-afd8-b7d915c224a2",
   "metadata": {},
   "source": [
    "# Chat messages example\n",
    "\n",
    "For integrating chat messages into Langchain - use ChatPromptTemplate with placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9edf0-80c4-4ce1-8f2b-18522aa9f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OpenAI templates instead - system, assistant, user\n",
    "template = ChatPromptTemplate([\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, how are you?\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I'm doing well, thank you for asking.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"{user_input}\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"placeholder\",  # Not sure if this will work\n",
    "        \"content\": \"{chat_history}\"\n",
    "    }\n",
    "])\n",
    "\n",
    "prompt_value = template.invoke(\n",
    "    {\n",
    "        \"user_input\": \"What is your name?\",\n",
    "        \"conversation\": [  # This is a normal template\n",
    "            (\"human\", \"Hi!\"),\n",
    "            (\"ai\", \"How can I assist you today?\"),\n",
    "            (\"human\", \"Can you make me an ice cream sundae?\"),\n",
    "            (\"ai\", \"No.\")\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bab03d-feb2-461d-affa-c888b66ff52e",
   "metadata": {},
   "source": [
    "# Hybrid Retrieval\n",
    "\n",
    "Combine a sparse search with a dense search - BM25 + Embeddings\n",
    "Reference Article: https://medium.com/etoai/hybrid-search-combining-bm25-and-semantic-search-for-better-results-with-lan-1358038fe7e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc1447-aef6-4609-b78f-fda3957a79f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2e7013d-717e-45c4-810a-efbfd9cc75d5",
   "metadata": {},
   "source": [
    "# Metadata Taggging - OpenAI\n",
    "\n",
    "For getting code metadata - e.g summaries and other info via an LLM - it enhances the metadata.\n",
    "https://python.langchain.com/docs/integrations/document_transformers/openai_metadata_tagger/\n",
    "\n",
    "Unfortunately this needs OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "effa5d09-5b62-44ea-ba2b-871360c7ec52",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     create_metadata_tagger,\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocuments\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_openai'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_transformers.openai_functions import (\n",
    "    create_metadata_tagger,\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91fb088d-4123-4c60-a3f7-3d4ce3835923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List\n",
    "from pydantic import BaseModel, Field\n",
    "class LLMGeneratedMetadata(BaseModel): \n",
    "    \"\"\"\n",
    "    Returns the blank schema for Lagnchain MetadataTagger to fill in via LLM. This is used to generate some parts of the metadata that cannot be made just by Langchain alone.\n",
    "\n",
    "    Metadata tagging can only be generated by OpenAI models in Langchain. Local models that supports OpenAI function calling is only somewhat supported by local-llm-function-calling.\n",
    "\n",
    "    Some of these fields are redundant, and already detected by AST. This is just kept as a potential use for cross validation, or just for filling in the gaps.\n",
    "    \"\"\"\n",
    "    block_type_llm: str = Field(description=\"Type of code block, either class/method/function/others. other block refers to when the code has no clear blocks such as when it exists in the root of the code file.\")\n",
    "    block_name_llm:  str = Field(description=\"name of block e.g class name, function name\")\n",
    "    block_args_llm: List[str] = Field(description=\"All argument variable names and types if any\")\n",
    "    return_var_llm:  str = Field(description=\"Returns name of return variable if clearcut, else if the return statement is complex such as an expression, return a variable name that sufficiently represents the return variable\")\n",
    "    functions_called_llm: List[str] = Field(description=\"List of other functions called, from internal code or external libraries\")\n",
    "    code_summary: str = Field(description=\"Summary of the code and its purpose\")  #  Code summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b2d6f8-7696-4237-83db-89075d8e0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be an OpenAI model that supports functions\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n",
    "\n",
    "document_transformer = create_metadata_tagger(metadata_schema=LLMGeneratedMetadata, llm=llm)\n",
    "\n",
    "# Add on to documents\n",
    "enhanced_documents = document_transformer.transform_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe877025-2bee-4f01-aa99-e4d2b3bc0dd3",
   "metadata": {},
   "source": [
    "# Metadata Tagging - Local\n",
    "Based on this https://local-llm-function-calling.readthedocs.io/en/latest/generation.html\n",
    "Uses the same function calling technique but with ollama models\n",
    "\n",
    "https://local-llm-function-calling.readthedocs.io/en/latest/constraining.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738c3ddf-0fc5-47d0-9cef-f960f249739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_llm_function_calling import Generator\n",
    "from local_llm_function_calling import Constrainer, JsonSchemaConstraint\n",
    "\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"llm_generated_metadata\",\n",
    "        \"description\": \"Generates metadata about the code document.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"block_type_llm\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Type of code block, either class/function/other. other block refers to when the code has no clear blocks such as when it exists in the root of the code file.\",\n",
    "                    \"enum\": [\"class\", \"function\", \"other\"],\n",
    "                },\n",
    "                \"block_name_llm\": {\n",
    "                    \"type\": \"string\", \n",
    "                    \"description\": \"\",\n",
    "                    \"maxLength\": 30\n",
    "                },\n",
    "                \"block_args_llm\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"description\": \"All argument variable names and types if any\",\n",
    "                },\n",
    "                \"return_var_llm\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Returns name of return variable if clearcut, else if the return statement is complex such as an expression, return a variable name that sufficiently represents the return variable\"\n",
    "                    \"maxLength\": 30\n",
    "                },\n",
    "                \"functions_called\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"description\": \"List of other functions called, from internal code or external libraries\"\n",
    "                },\n",
    "                \"code_summary\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Summary of the code and its purpose\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"code_summary\"],\n",
    "        },\n",
    "    },   \n",
    "]\n",
    "\n",
    "constraint = JsonSchemaConstraint(schema)\n",
    "constrainer = Constrainer(HuggingfaceModel(\"gpt2\"))\n",
    "raw_json = constrainer.generate(\"Prefix.\\n\", constraint, max_len=100)\n",
    "truncated_json = raw_json[:constraint.validate(raw_json).end_index]  # Remove the prefix\n",
    "\n",
    "# Now the metadata can be added manually to the metadata of the documents - similar to transform_documents(documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
